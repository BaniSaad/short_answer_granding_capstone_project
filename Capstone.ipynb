{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project - Short Answer Grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m print_function\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msource.utils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m generate_data\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn.externals\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m joblib\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mkeras.layers.core\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Dropout\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mkeras.models\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Sequential\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mkeras.layers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Dense, Embedding\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mkeras.layers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m LSTM\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn.linear_model\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m LinearRegression\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn.svm\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m SVC\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn.neural_network\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m MLPClassifier\n",
      "\n",
      "\n",
      "\u001b[37m# Provided model load function\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\n",
      "    \u001b[33m\"\"\"Load model from the model_dir. This is the same model that is saved\u001b[39;49;00m\n",
      "\u001b[33m    in the main if statement.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# load using joblib\u001b[39;49;00m\n",
      "    model = joblib.load(os.path.join(model_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mmodel.joblib\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mDone loading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "    \u001b[37m# All of the model parameters and training parameters are sent as arguments\u001b[39;49;00m\n",
      "    \u001b[37m# when this script is executed, during a training job\u001b[39;49;00m\n",
      "\n",
      "    \u001b[37m# Here we set up an argument parser to easily access the parameters\u001b[39;49;00m\n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    \u001b[37m# SageMaker parameters, like the directories for training data and saving models; set automatically\u001b[39;49;00m\n",
      "    \u001b[37m# Do not need to change\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output-data-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--data-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAINING\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m200\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--embedding_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m30\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--lstm_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m100\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--dropout\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.2\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--optimizer\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33madam\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "    \u001b[37m# args holds all passed-in arguments\u001b[39;49;00m\n",
      "    args = parser.parse_args()\n",
      "\n",
      "    \u001b[34mprint\u001b[39;49;00m(f\u001b[33m\"\u001b[39;49;00m\u001b[33mepochs={args.epochs} embedding_size={args.embedding_size} lstm_size={args.lstm_size}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Read in csv training file\u001b[39;49;00m\n",
      "    training_dir = args.data_dir\n",
      "    train_data = pd.read_csv(os.path.join(training_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mtrain.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), header=\u001b[36mNone\u001b[39;49;00m, names=\u001b[36mNone\u001b[39;49;00m)\n",
      "    vocab = pd.read_csv(os.path.join(training_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mvocab.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), header=\u001b[36mNone\u001b[39;49;00m, names=\u001b[36mNone\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Labels are in the first column\u001b[39;49;00m\n",
      "    train_y = train_data.iloc[:, \u001b[34m0\u001b[39;49;00m]\n",
      "    train_x = train_data.iloc[:, \u001b[34m1\u001b[39;49;00m:]\n",
      "    max_length = train_x.values.shape[\u001b[34m1\u001b[39;49;00m]\n",
      "\n",
      "    \u001b[37m# Build Model\u001b[39;49;00m\n",
      "    model = Sequential()\n",
      "    model.add(Embedding(\u001b[36mlen\u001b[39;49;00m(vocab), args.embedding_size, input_length=max_length))\n",
      "    model.add(Dropout(args.dropout))\n",
      "    model.add(LSTM(args.lstm_size, return_sequences=\u001b[36mFalse\u001b[39;49;00m, input_shape=(max_length,)))\n",
      "    model.add(Dropout(args.dropout))\n",
      "    model.add(Dense(\u001b[34m1\u001b[39;49;00m, activation=\u001b[33m\"\u001b[39;49;00m\u001b[33mlinear\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "    model.compile(optimizer=args.optimizer, loss=\u001b[33m'\u001b[39;49;00m\u001b[33mmean_squared_error\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, metrics=[\u001b[33m'\u001b[39;49;00m\u001b[33macc\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\n",
      "\n",
      "    \u001b[37m# Train the model\u001b[39;49;00m\n",
      "    model.fit(train_x, train_y, epochs=args.epochs, verbose=\u001b[34m1\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m## --- End of your code  --- ##\u001b[39;49;00m\n",
      "\n",
      "    \u001b[37m# Save the trained model\u001b[39;49;00m\n",
      "    joblib.dump(model, os.path.join(args.model_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mmodel.joblib\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n"
     ]
    }
   ],
   "source": [
    "# source code for SKLearn custom train.py\n",
    "!pygmentize source/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "# session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# create an S3 bucket\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be the name of directory you created to save your features data\n",
    "data_dir = 'data/seb'\n",
    "\n",
    "# set prefix, a descriptive name for a directory  \n",
    "prefix = 'sagemaker/short_answer'\n",
    "\n",
    "# upload all data to S3\n",
    "input_data = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Basic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "# instantiate a pytorch estimator\n",
    "# estimator = SKLearn(entry_point='train.py',\n",
    "#                     source_dir='source', # this should be just \"source\" for your code\n",
    "#                     role=role,\n",
    "#                     train_instance_count=1,\n",
    "#                     train_instance_type='ml.c4.xlarge',\n",
    "#                     hyperparameters = {\n",
    "#                         'epochs': 200,\n",
    "#                         'embedding_size': 30,\n",
    "#                         'lstm_size': 100,\n",
    "#                         'dropout': 0.2,\n",
    "#                         'optimizer': 'adam'\n",
    "#                     }\n",
    "#                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "estimator = TensorFlow(entry_point='source/train.py', \n",
    "                       role=role,\n",
    "                       train_instance_count=1, \n",
    "                       train_instance_type='ml.c4.xlarge',\n",
    "                       framework_version='1.12.0', \n",
    "                       py_version='py3',\n",
    "                       script_mode=True,\n",
    "                       hyperparameters = {\n",
    "                        'epochs': 200,\n",
    "                        'embedding_size': 30,\n",
    "                        'lstm_size': 100,\n",
    "                        'dropout': 0.2,\n",
    "                        'optimizer': 'adam'\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.training.adam import AdamOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-25 21:06:22 Starting - Starting the training job...\n",
      "2019-06-25 21:06:25 Starting - Launching requested ML instances......\n",
      "2019-06-25 21:07:32 Starting - Preparing the instances for training......\n",
      "2019-06-25 21:08:46 Downloading - Downloading input data\n",
      "2019-06-25 21:08:46 Training - Downloading the training image..\n",
      "\u001b[31m2019-06-25 21:09:03,625 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[31m2019-06-25 21:09:03,631 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m2019-06-25 21:09:03,956 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m2019-06-25 21:09:03,972 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m2019-06-25 21:09:03,984 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[31mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[31m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"optimizer\": \"adam\",\n",
      "        \"dropout\": 0.2,\n",
      "        \"embedding_size\": 30,\n",
      "        \"lstm_size\": 100,\n",
      "        \"model_dir\": \"s3://sagemaker-us-east-1-399712746635/sagemaker-tensorflow-scriptmode-2019-06-23-00-14-46-505/model\",\n",
      "        \"epochs\": 200\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-tensorflow-scriptmode-2019-06-25-21-06-22-369\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-399712746635/sagemaker-tensorflow-scriptmode-2019-06-25-21-06-22-369/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[31m}\n",
      "\u001b[0m\n",
      "\u001b[31mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[31mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[31mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[31mSM_HPS={\"dropout\":0.2,\"embedding_size\":30,\"epochs\":200,\"lstm_size\":100,\"model_dir\":\"s3://sagemaker-us-east-1-399712746635/sagemaker-tensorflow-scriptmode-2019-06-23-00-14-46-505/model\",\"optimizer\":\"adam\"}\u001b[0m\n",
      "\u001b[31mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[31mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[31mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[31mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[31mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[31mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[31mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[31mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[31mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[31mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[31mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[31mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[31mSM_MODULE_DIR=s3://sagemaker-us-east-1-399712746635/sagemaker-tensorflow-scriptmode-2019-06-25-21-06-22-369/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[31mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"dropout\":0.2,\"embedding_size\":30,\"epochs\":200,\"lstm_size\":100,\"model_dir\":\"s3://sagemaker-us-east-1-399712746635/sagemaker-tensorflow-scriptmode-2019-06-23-00-14-46-505/model\",\"optimizer\":\"adam\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-tensorflow-scriptmode-2019-06-25-21-06-22-369\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-399712746635/sagemaker-tensorflow-scriptmode-2019-06-25-21-06-22-369/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[31mSM_USER_ARGS=[\"--dropout\",\"0.2\",\"--embedding_size\",\"30\",\"--epochs\",\"200\",\"--lstm_size\",\"100\",\"--model_dir\",\"s3://sagemaker-us-east-1-399712746635/sagemaker-tensorflow-scriptmode-2019-06-23-00-14-46-505/model\",\"--optimizer\",\"adam\"]\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[31mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[31mSM_HP_OPTIMIZER=adam\u001b[0m\n",
      "\u001b[31mSM_HP_DROPOUT=0.2\u001b[0m\n",
      "\u001b[31mSM_HP_EMBEDDING_SIZE=30\u001b[0m\n",
      "\u001b[31mSM_HP_LSTM_SIZE=100\u001b[0m\n",
      "\u001b[31mSM_HP_MODEL_DIR=s3://sagemaker-us-east-1-399712746635/sagemaker-tensorflow-scriptmode-2019-06-23-00-14-46-505/model\u001b[0m\n",
      "\u001b[31mSM_HP_EPOCHS=200\u001b[0m\n",
      "\u001b[31mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[31mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[31m/usr/bin/python train.py --dropout 0.2 --embedding_size 30 --epochs 200 --lstm_size 100 --model_dir s3://sagemaker-us-east-1-399712746635/sagemaker-tensorflow-scriptmode-2019-06-23-00-14-46-505/model --optimizer adam\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[31mepochs=200 embedding_size=30 lstm_size=100\u001b[0m\n",
      "\n",
      "2019-06-25 21:09:00 Training - Training image download completed. Training in progress.\u001b[31mValidation_loss:0.27569736852202303;Validation_accuracy:0.720930231171985\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:Export includes no default signature!\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:Export includes no default signature!\u001b[0m\n",
      "\u001b[31mSave Keras Model result: b\"/opt/ml/model/temp-b'1561497056'\"\u001b[0m\n",
      "\u001b[31m2019-06-25 21:11:00,020 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2019-06-25 21:11:07 Uploading - Uploading generated training model\n",
      "2019-06-25 21:11:07 Completed - Training job completed\n",
      "Billable seconds: 160\n",
      "CPU times: user 642 ms, sys: 15.8 ms, total: 658 ms\n",
      "Wall time: 5min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train estimator on S3 training data\n",
    "estimator.fit(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low Level Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will need to know the name of the container that we want to use for training. SageMaker provides\n",
    "# a nice utility method to construct this for us.\n",
    "container = get_image_uri(session.boto_region_name, 'xgboost')\n",
    "\n",
    "# We now specify the parameters we wish to use for our training job\n",
    "training_params = {}\n",
    "\n",
    "# We need to specify the permissions that this training job will have. For our purposes we can use\n",
    "# the same permissions that our current SageMaker session has.\n",
    "training_params['RoleArn'] = role\n",
    "\n",
    "# Here we describe the algorithm we wish to use. The most important part is the container which\n",
    "# contains the training code.\n",
    "training_params['AlgorithmSpecification'] = {\n",
    "    \"TrainingImage\": container,\n",
    "    \"TrainingInputMode\": \"File\"\n",
    "}\n",
    "\n",
    "# We also need to say where we would like the resulting model artifacts stored.\n",
    "training_params['OutputDataConfig'] = {\n",
    "    \"S3OutputPath\": \"s3://\" + session.default_bucket() + \"/\" + prefix + \"/output\"\n",
    "}\n",
    "\n",
    "# We also need to set some parameters for the training job itself. Namely we need to describe what sort of\n",
    "# compute instance we wish to use along with a stopping condition to handle the case that there is\n",
    "# some sort of error and the training script doesn't terminate.\n",
    "training_params['ResourceConfig'] = {\n",
    "    \"InstanceCount\": 1,\n",
    "    \"InstanceType\": \"ml.m4.xlarge\",\n",
    "    \"VolumeSizeInGB\": 5\n",
    "}\n",
    "    \n",
    "training_params['StoppingCondition'] = {\n",
    "    \"MaxRuntimeInSeconds\": 86400\n",
    "}\n",
    "\n",
    "# Next we set the algorithm specific hyperparameters. In this case, since we are setting up\n",
    "# a training job which will serve as the base training job for the eventual hyperparameter\n",
    "# tuning job, we only specify the _static_ hyperparameters. That is, the hyperparameters that\n",
    "# we do _not_ want SageMaker to change.\n",
    "training_params['StaticHyperParameters'] = {\n",
    "    \"gamma\": \"4\",\n",
    "    \"subsample\": \"0.8\",\n",
    "    \"objective\": \"reg:linear\",\n",
    "    \"early_stopping_rounds\": \"10\",\n",
    "    \"num_round\": \"200\"\n",
    "}\n",
    "\n",
    "# Now we need to tell SageMaker where the data should be retrieved from.\n",
    "training_params['InputDataConfig'] = [\n",
    "    {\n",
    "        \"ChannelName\": \"train\",\n",
    "        \"DataSource\": {\n",
    "            \"S3DataSource\": {\n",
    "                \"S3DataType\": \"S3Prefix\",\n",
    "                \"S3Uri\": train_location,\n",
    "                \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "            }\n",
    "        },\n",
    "        \"ContentType\": \"csv\",\n",
    "        \"CompressionType\": \"None\"\n",
    "    },\n",
    "    {\n",
    "        \"ChannelName\": \"validation\",\n",
    "        \"DataSource\": {\n",
    "            \"S3DataSource\": {\n",
    "                \"S3DataType\": \"S3Prefix\",\n",
    "                \"S3Uri\": val_location,\n",
    "                \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "            }\n",
    "        },\n",
    "        \"ContentType\": \"csv\",\n",
    "        \"CompressionType\": \"None\"\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "=[\n",
    "    {\n",
    "        \"Name\": \"loss\",\n",
    "        \"Regex\": \"Loss = (.*?);\",\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"ganloss\",\n",
    "        \"Regex\": \"GAN_loss=(.*?);\",\n",
    "    },    \n",
    "    {\n",
    "        \"Name\": \"discloss\",\n",
    "        \"Regex\": \"disc_train_loss=(.*?);\",\n",
    "    },    \n",
    "    {\n",
    "        \"Name\": \"disc-combined\",\n",
    "        \"Regex\": \"disc-combined=(.*?);\",\n",
    "    },    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypertuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, make sure to import the relevant objects used to construct the tuner\n",
    "from sagemaker.tuner import IntegerParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "tf_hyperparameter_tuner = HyperparameterTuner(estimator = estimator, # The estimator object to use as the basis for the training jobs.\n",
    "                                               objective_metric_name = 'Validation_loss', # The metric used to compare trained models.\n",
    "                                               objective_type = 'Minimize', # Whether we wish to minimize or maximize the metric.\n",
    "                                               max_jobs = 6, # The total number of models to train\n",
    "                                               max_parallel_jobs = 3, # The number of models to train in parallel\n",
    "                                               hyperparameter_ranges = {\n",
    "                                                    'epochs': IntegerParameter(50, 300),\n",
    "                                                    'dropout': ContinuousParameter(0.05, 0.5),\n",
    "                                                    'embeding_size': IntegerParameter(10, 200),\n",
    "                                                    'lstm_size': IntegerParameter(10, 200)\n",
    "                                               })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-399712746635/sagemaker/short_answer/train.csv\n"
     ]
    }
   ],
   "source": [
    "print('{}/train.csv'.format(input_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.s3_input(s3_data='{}/train.csv'.format(input_data), content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data='{}/test.csv'.format(input_data), content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the CreateHyperParameterTuningJob operation: A metric is required for this hyperparameter tuning job objective. Provide a metric in the metric definitions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-fbe49eeed7f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf_hyperparameter_tuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms3_input_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'validation'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms3_input_validation\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/tuner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, job_name, include_cls_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_for_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_cls_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude_cls_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_tuning_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TuningJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/tuner.py\u001b[0m in \u001b[0;36mstart_new\u001b[0;34m(cls, tuner, inputs)\u001b[0m\n\u001b[1;32m    658\u001b[0m             \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencrypt_inter_container_traffic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m         \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtuner_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_job_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mtune\u001b[0;34m(self, job_name, strategy, objective_type, objective_metric_name, max_jobs, max_parallel_jobs, parameter_ranges, static_hyperparameters, input_mode, metric_definitions, role, input_config, output_config, resource_config, stop_condition, tags, warm_start_config, enable_network_isolation, image, algorithm_arn, early_stopping_type, encrypt_inter_container_traffic, vpc_config)\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Creating hyperparameter tuning job with name: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tune request: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtune_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_hyper_parameter_tuning_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtune_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstop_tuning_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    356\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    659\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the CreateHyperParameterTuningJob operation: A metric is required for this hyperparameter tuning job objective. Provide a metric in the metric definitions."
     ]
    }
   ],
   "source": [
    "tf_hyperparameter_tuner.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_hyperparameter_tuner.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1,\n",
    "                             # instance_type='ml.c5.xlarge',\n",
    "                             instance_type='ml.t2.medium',\n",
    "                             endpoint_type='tensorflow-serving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-tensorflow-scriptmode-2019-06-24-21-36-44-186\n",
      "sagemaker-us-east-1-399712746635\n",
      "s3://sagemaker-us-east-1-399712746635/sagemaker-tensorflow-scriptmode-2019-06-24-21-36-44-186/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "job_name = estimator._current_job_name\n",
    "print(job_name)\n",
    "print(bucket)\n",
    "model_data = os.path.join('s3://',bucket,job_name,'output/model.tar.gz')\n",
    "print(model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow.serving import Model\n",
    "model = Model(model_data=model_data, role=role)\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.c5.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlowModel\n",
    "\n",
    "tf_model = TensorFlowModel(model_data=model_data,\n",
    "                           role=role,\n",
    "                           py_version='py3',\n",
    "                           entry_point='source/train.py',\n",
    "                           name='short-answer')\n",
    "\n",
    "predictor = tf_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def evaluate(predictor, test_features, test_labels, verbose=True):\n",
    "    \"\"\"\n",
    "    Evaluate a model on a test set given the prediction endpoint.  \n",
    "    Return binary classification metrics.\n",
    "    :param predictor: A prediction endpoint\n",
    "    :param test_features: Test features\n",
    "    :param test_labels: Class labels for test data\n",
    "    :param verbose: If True, prints a table of all performance metrics\n",
    "    :return: A dictionary of performance metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    # rounding and squeezing array\n",
    "    test_preds = np.squeeze(predictor.predict(test_features)['predictions'])\n",
    "    # Normalized to range 0 to 1 if needed\n",
    "    if test_preds.max() > 1:\n",
    "        test_preds = test_preds * (1 / test_preds.max())\n",
    "    test_preds = np.round(test_preds)    \n",
    "    # calculate true positives, false positives, true negatives, false negatives\n",
    "    tp = np.logical_and(test_labels, test_preds).sum()\n",
    "    fp = np.logical_and(1-test_labels, test_preds).sum()\n",
    "    tn = np.logical_and(1-test_labels, 1-test_preds).sum()\n",
    "    fn = np.logical_and(test_labels, 1-test_preds).sum()\n",
    "    \n",
    "    # calculate binary classification metrics\n",
    "    recall = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "    \n",
    "    # print metrics\n",
    "    if verbose:\n",
    "        print(pd.crosstab(test_labels, test_preds, rownames=['actuals'], colnames=['predictions']))\n",
    "        print(\"\\n{:<11} {:.3f}\".format('Recall:', recall))\n",
    "        print(\"{:<11} {:.3f}\".format('Precision:', precision))\n",
    "        print(\"{:<11} {:.3f}\".format('Accuracy:', accuracy))\n",
    "        print()\n",
    "        \n",
    "    return tp, fp, fn, tn, precision, recall, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(os.path.join(data_dir, \"test.csv\"), header=None, names=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = test_data.iloc[:, 0].values\n",
    "test_x = test_data.iloc[:, 1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01876325  0.02068142  0.02653523  0.35195724  0.72061773  0.87722571\n",
      "  0.01853903  0.26202348 -0.14137819  0.08305078  0.0955543   0.01703948\n",
      "  0.04691585  0.13622449  0.99101351  0.87536935  1.          0.80075494\n",
      "  0.02298404  0.029142    0.0258112   0.29992913  0.23560735  0.03663976\n",
      "  0.55473981  0.45330276  0.02148174  0.36156186  0.15412955  0.07393319\n",
      "  0.03006211  0.02122484  0.92390228  0.02723624  0.02116076  0.04249387\n",
      "  0.03247419  0.02789197  0.0264619   0.06763694  0.20393909  0.03341093\n",
      "  0.8354164 ]\n"
     ]
    }
   ],
   "source": [
    "# First: generate predicted, class labels\n",
    "test_y_preds = np.squeeze(predictor.predict(test_x)['predictions'])\n",
    "#normalize if needed\n",
    "if max(test_y_preds) > 1:\n",
    "    test_y_preds = test_y_preds * (1 / test_y_preds.max())\n",
    "print(test_y_preds)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions  -0.0   1.0\n",
      "actuals                \n",
      "0.0            23     3\n",
      "1.0            11     6\n",
      "\n",
      "Recall:     0.353\n",
      "Precision:  0.667\n",
      "Accuracy:   0.674\n",
      "\n",
      "0.6744186046511628\n",
      "    raw_predicted  predicted  actual\n",
      "0       -0.018763       -0.0     0.0\n",
      "1        0.020681        0.0     1.0\n",
      "2        0.026535        0.0     0.0\n",
      "3        0.351957        0.0     0.0\n",
      "4        0.720618        1.0     1.0\n",
      "5        0.877226        1.0     1.0\n",
      "6        0.018539        0.0     0.0\n",
      "7        0.262023        0.0     0.0\n",
      "8       -0.141378       -0.0     1.0\n",
      "9        0.083051        0.0     0.0\n",
      "10       0.095554        0.0     1.0\n",
      "11       0.017039        0.0     0.0\n",
      "12       0.046916        0.0     0.0\n",
      "13       0.136224        0.0     0.0\n",
      "14       0.991014        1.0     1.0\n",
      "15       0.875369        1.0     0.0\n",
      "16       1.000000        1.0     1.0\n",
      "17       0.800755        1.0     0.0\n",
      "18       0.022984        0.0     0.0\n",
      "19       0.029142        0.0     1.0\n",
      "20       0.025811        0.0     0.0\n",
      "21       0.299929        0.0     1.0\n",
      "22       0.235607        0.0     1.0\n",
      "23       0.036640        0.0     0.0\n",
      "24       0.554740        1.0     1.0\n",
      "25       0.453303        0.0     0.0\n",
      "26       0.021482        0.0     1.0\n",
      "27       0.361562        0.0     1.0\n",
      "28       0.154130        0.0     0.0\n",
      "29       0.073933        0.0     1.0\n",
      "30       0.030062        0.0     0.0\n",
      "31       0.021225        0.0     0.0\n",
      "32       0.923902        1.0     1.0\n",
      "33       0.027236        0.0     0.0\n",
      "34       0.021161        0.0     0.0\n",
      "35       0.042494        0.0     0.0\n",
      "36       0.032474        0.0     0.0\n",
      "37       0.027892        0.0     0.0\n",
      "38       0.026462        0.0     1.0\n",
      "39       0.067637        0.0     0.0\n",
      "40       0.203939        0.0     0.0\n",
      "41       0.033411        0.0     1.0\n",
      "42       0.835416        1.0     0.0\n"
     ]
    }
   ],
   "source": [
    "# Second: calculate the test accuracy\n",
    "tp, fp, fn, tn, precision, recall, accuracy = evaluate(predictor, test_x, test_y)\n",
    "\n",
    "print(accuracy)\n",
    "\n",
    "## print out the array of predicted and true labels, if you want\n",
    "results = pd.concat([pd.DataFrame(test_y_preds), pd.DataFrame(np.round(test_y_preds)), pd.DataFrame(test_y)], axis=1)\n",
    "results.columns = ['raw_predicted','predicted','actual']\n",
    "# print('\\nPredicted class labels: ')\n",
    "# print(test_y_preds)\n",
    "# print('\\nTrue class labels: ')\n",
    "# print(test_y.values)\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
